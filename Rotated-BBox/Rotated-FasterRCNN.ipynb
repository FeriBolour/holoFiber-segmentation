{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6666945",
   "metadata": {},
   "source": [
    "# Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554be14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import detectron2\n",
    "import contextlib\n",
    "import datetime\n",
    "import io\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import copy,torch,torchvision\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as X\n",
    "import math\n",
    "from itertools import repeat\n",
    "import re\n",
    "import shutil\n",
    "import io\n",
    "import ast\n",
    "\n",
    "from fvcore.common.file_io import PathManager\n",
    "from fvcore.common.timer import Timer\n",
    "\n",
    "from detectron2.structures import Boxes, BoxMode, PolygonMasks\n",
    "from detectron2.config import *\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.data import transforms as T\n",
    "from detectron2.data import detection_utils as utils\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader, build_detection_train_loader\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.engine import DefaultTrainer, DefaultPredictor\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.evaluation import RotatedCOCOEvaluator,DatasetEvaluators, inference_on_dataset, coco_evaluation,DatasetEvaluator\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from platform import python_version\n",
    "\n",
    "import glob\n",
    "import time\n",
    "import shutil\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import concurrent.futures\n",
    "\n",
    "import torch\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "from torch.utils.cpp_extension import CUDA_HOME\n",
    "print(torch.cuda.is_available(), CUDA_HOME)\n",
    "\n",
    "setup_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102ad9fb",
   "metadata": {},
   "source": [
    "# Custom Function for Preparing Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08c314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rbbox(mask):\n",
    "    import cv2\n",
    "    cnts, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    rbbox = cv2.minAreaRect(cnts[0])\n",
    "    return rbbox\n",
    "\n",
    "\n",
    "\n",
    "def make_rbbox_cotton_dicts(Train_data_path, image_id = 1):\n",
    "\n",
    "    padded_seg_dicts = make_seg_cotton_dicts(Train_data_path)\n",
    "\n",
    "    dataset_list = []\n",
    "    for file in padded_seg_dicts:\n",
    "\n",
    "        img_height = file['height']\n",
    "        img_width = file['width']\n",
    "        img_path = file['file_name']\n",
    "        frame_name = file['fr_name']\n",
    "\n",
    "        dict_holder = {}\n",
    "        dict_holder[\"file_name\"] = img_path\n",
    "        dict_holder[\"height\"] =  img_height\n",
    "        dict_holder[\"width\"] = img_width\n",
    "        dict_holder[\"image_id\"] = image_id\n",
    "        dict_holder[\"fr_name\"] = frame_name\n",
    "\n",
    "        # loop over each instance in current image and save annotations dictionary in a list\n",
    "        annotations = []\n",
    "        for index,variable in enumerate(file['annotations']):\n",
    "            category = variable['category_id']\n",
    "            segment = variable['segmentation']\n",
    "            mymask = detectron2.structures.polygons_to_bitmask(segment, img_height,img_width)\n",
    "            mymask = 255*mymask\n",
    "            rbbox = get_rbbox((mymask).astype('uint8'))\n",
    "            cent_x = rbbox[0][0]\n",
    "            cent_y = rbbox[0][1]\n",
    "            w = rbbox[1][0]\n",
    "            h = rbbox[1][1]\n",
    "            angle = rbbox[2]\n",
    "#             if h > w:\n",
    "#                 angle = 90-angle\n",
    "#             else:\n",
    "            angle = -angle # -angle works best (for now)\n",
    "            bbox = [cent_x, cent_y, w, h, angle]\n",
    "            bbox_mode = detectron2.structures.BoxMode(4) # box_mode = 4 --> (x_cent,y_cent,w,h,a)\n",
    "            dict_annot = {\n",
    "                            \"bbox\": bbox,\n",
    "                            \"bbox_mode\": bbox_mode,\n",
    "                            \"category_id\": category,\n",
    "                        }\n",
    "            annotations.append(dict_annot)\n",
    "\n",
    "        dict_holder[\"annotations\"] = annotations\n",
    "\n",
    "        if 'train' in Train_data_path:\n",
    "                    dataset_list.append(dict_holder)\n",
    "                    image_id += 1\n",
    "        else:\n",
    "            if 'aug' in frame_name:\n",
    "                dataset_list.append(dict_holder)\n",
    "                image_id += 1\n",
    "                \n",
    "    return dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1c790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_data_path = 'train_average'\n",
    "Base_path = 'Cotton Fiber Project'\n",
    "rbbox_train_dicts = make_rbbox_cotton_dicts(Train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93e2ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in [\"train_average\"]: #,,\"val\",\"test\" (enter inside list for val data creation)\n",
    "    DatasetCatalog.register(\"CFH_\" + d,lambda d=d: make_seg_cotton_dicts(os.path.join(Base_path,d)))\n",
    "    MetadataCatalog.get(\"CFH_\" + d).thing_classes=[\"fiber\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1a2253",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_train = MetadataCatalog.get(\"CFH_train_average\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d75d90",
   "metadata": {},
   "source": [
    "# Custom Dataset Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3f2d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_transform_instance_annotations(annotation, transforms, image_size, *, keypoint_hflip_indices=None):\n",
    "    if annotation[\"bbox_mode\"] == BoxMode.XYWHA_ABS:\n",
    "        annotation[\"bbox\"] = transforms.apply_rotated_box(np.asarray([annotation[\"bbox\"]]))[0]\n",
    "    else:\n",
    "        bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n",
    "        # Note that bbox is 1d (per-instance bounding box)\n",
    "        annotation[\"bbox\"] = transforms.apply_box([bbox])[0]\n",
    "        annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n",
    "\n",
    "    return annotation\n",
    "\n",
    "def mapper(dataset_dict):\n",
    "    # Implement a mapper, similar to the default DatasetMapper, but with our own customizations\n",
    "    dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n",
    "    image = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")\n",
    "    image, transforms = T.apply_transform_gens([T.Resize((800, 800))], image)\n",
    "    dataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1).astype(\"float32\"))\n",
    "\n",
    "    annos = [\n",
    "      my_transform_instance_annotations(obj, transforms, image.shape[:2]) \n",
    "      for obj in dataset_dict.pop(\"annotations\")\n",
    "      if obj.get(\"iscrowd\", 0) == 0\n",
    "    ]\n",
    "    instances = utils.annotations_to_instances_rotated(annos, image.shape[:2])\n",
    "    dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5466acfa",
   "metadata": {},
   "source": [
    "# Custom Trainer and Visualizer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a231b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name):\n",
    "        output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "        evaluators = [RotatedCOCOEvaluator(dataset_name, cfg, True, output_folder)]\n",
    "        return DatasetEvaluators(evaluators)\n",
    "      \n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "        return build_detection_train_loader(cfg, mapper=mapper)\n",
    "\n",
    "class RotatedPredictor(DefaultPredictor):\n",
    "    def __init__(self, cfg):\n",
    "        \n",
    "        self.cfg = cfg.clone()  # cfg can be modified by model\n",
    "        self.model = trainer.model\n",
    "        self.model.eval()\n",
    "\n",
    "        self.transform_gen = T.ResizeShortestEdge(\n",
    "            [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST\n",
    "        )\n",
    "\n",
    "        self.input_format = cfg.INPUT.FORMAT\n",
    "        assert self.input_format in [\"RGB\", \"BGR\"], self.input_format\n",
    "\n",
    "    def __call__(self, original_image):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            original_image (np.ndarray): an image of shape (H, W, C) (in BGR order).\n",
    "        Returns:\n",
    "            predictions (dict):\n",
    "                the output of the model for one image only.\n",
    "                See :doc:`/tutorials/models` for details about the format.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():  # https://github.com/sphinx-doc/sphinx/issues/4258\n",
    "            # Apply pre-processing to image.\n",
    "            if self.input_format == \"RGB\":\n",
    "                # whether the model expects BGR inputs or RGB\n",
    "                original_image = original_image[:, :, ::-1]\n",
    "            height, width = original_image.shape[:2]\n",
    "            image = self.transform_gen.get_transform(original_image).apply_image(original_image)\n",
    "            image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
    "\n",
    "            inputs = {\"image\": image, \"height\": height, \"width\": width}\n",
    "            predictions = self.model([inputs])[0]\n",
    "            return predictions\n",
    "\n",
    "# As of 0.3 the XYWHA_ABS box is not supported in the visualizer, this is fixed in master branch atm (19/11/20)\n",
    "class myVisualizer(Visualizer):\n",
    "  \n",
    "    def draw_dataset_dict(self, dic):\n",
    "        annos = dic.get(\"annotations\", None)\n",
    "        if annos:\n",
    "            if \"segmentation\" in annos[0]:\n",
    "                masks = [x[\"segmentation\"] for x in annos]\n",
    "            else:\n",
    "                masks = None\n",
    "            if \"keypoints\" in annos[0]:\n",
    "                keypts = [x[\"keypoints\"] for x in annos]\n",
    "                keypts = np.array(keypts).reshape(len(annos), -1, 3)\n",
    "            else:\n",
    "                keypts = None\n",
    "\n",
    "            boxes = [BoxMode.convert(x[\"bbox\"], x[\"bbox_mode\"], BoxMode.XYWHA_ABS) for x in annos]\n",
    "\n",
    "            labels = [x[\"category_id\"] for x in annos]\n",
    "            names = self.metadata.get(\"thing_classes\", None)\n",
    "            if names:\n",
    "                labels = [names[i] for i in labels]\n",
    "            labels = [\n",
    "                \"{}\".format(i) + (\"|crowd\" if a.get(\"iscrowd\", 0) else \"\")\n",
    "                for i, a in zip(labels, annos)\n",
    "            ]\n",
    "            self.overlay_instances(labels=labels, boxes=boxes, masks=masks, keypoints=keypts)\n",
    "\n",
    "        sem_seg = dic.get(\"sem_seg\", None)\n",
    "        if sem_seg is None and \"sem_seg_file_name\" in dic:\n",
    "            sem_seg = cv2.imread(dic[\"sem_seg_file_name\"], cv2.IMREAD_GRAYSCALE)\n",
    "        if sem_seg is not None:\n",
    "            self.draw_sem_seg(sem_seg, area_threshold=0, alpha=0.5)\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25a4ad2",
   "metadata": {},
   "source": [
    "## Function to Save the Detectron2 Config into Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b8fbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cfg2yaml(cfg):\n",
    "    \n",
    "    with open(cfg.OUTPUT_DIR + \"/Config.txt\", 'w') as file:\n",
    "        file.write(cfg.dump())\n",
    "    \n",
    "    os.rename(cfg.OUTPUT_DIR + \"/Config.txt\", cfg.OUTPUT_DIR + \"/Config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acbdb95",
   "metadata": {},
   "source": [
    "# Setup Detectron2's Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6eefab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "\n",
    "cfg.OUTPUT_DIR = 'FasterRCNN Test'\n",
    "\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\") # Let training initialize from model zoo\n",
    "cfg.DATASETS.TRAIN = (\"CFH_train_average_padded_rotated\",)\n",
    "cfg.DATASETS.TEST = ()\n",
    "\n",
    "cfg.MODEL.MASK_ON=False\n",
    "cfg.MODEL.PROPOSAL_GENERATOR.NAME = \"RRPN\"\n",
    "cfg.MODEL.RPN.HEAD_NAME = \"StandardRPNHead\"\n",
    "cfg.MODEL.RPN.BBOX_REG_WEIGHTS = (10,10,5,5,1)\n",
    "cfg.MODEL.ANCHOR_GENERATOR.NAME = \"RotatedAnchorGenerator\"\n",
    "cfg.MODEL.ANCHOR_GENERATOR.ANGLES = [[-90,-72,-54,-36,-18,0,18,36,54,72,90]]\n",
    "# cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.9 \n",
    "cfg.MODEL.ROI_HEADS.NAME = \"RROIHeads\"\n",
    "# cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512   #this is far lower than usual.  \n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES =1\n",
    "cfg.MODEL.ROI_BOX_HEAD.POOLER_TYPE = \"ROIAlignRotated\"\n",
    "cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS = (10,10,5,5,1)\n",
    "# cfg.MODEL.ROI_BOX_HEAD.NUM_CONV=4\n",
    "# cfg.MODEL.ROI_MASK_HEAD.NUM_CONV=8\n",
    "cfg.SOLVER.IMS_PER_BATCH = 14 #can be up to  24 for a p100 (6 default)\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD=2500\n",
    "cfg.SOLVER.BASE_LR = 0.00015\n",
    "# cfg.SOLVER.GAMMA=0.5\n",
    "cfg.SOLVER.STEPS=(17500, 19000)\n",
    "cfg.SOLVER.MAX_ITER=20000\n",
    "\n",
    "\n",
    "cfg.DATALOADER.NUM_WORKERS = 1\n",
    "cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS = True \n",
    "# cfg.DATALOADER.SAMPLER_TRAIN= \"RepeatFactorTrainingSampler\"\n",
    "# cfg.DATALOADER.REPEAT_THRESHOLD=0.01\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)#lets just check our output dir exists\n",
    "# cfg.MODEL.BACKBONE.FREEZE_AT=6\n",
    "cfg2yaml(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e824a425",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MyTrainer(cfg) \n",
    "trainer.resume_or_load(resume=True)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4643ffc3",
   "metadata": {},
   "source": [
    "# Custom COCO Evaluator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d944e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bug in RotatedCOCOEvaluator where it gets passed img_ids\n",
    "class MyRotatedCOCOEvaluator(RotatedCOCOEvaluator):\n",
    "    def _eval_predictions(self, tasks, predictions, img_ids=None):\n",
    "        super()._eval_predictions(tasks, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb538c13",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73e8342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create coco evaluator, but use the default detectron2 data format for generation, make sure ids overlap\n",
    "evaluator = MyRotatedCOCOEvaluator(\"CFH_train_average_padded_rotated\", cfg, False, output_dir=cfg.OUTPUT_DIR)\n",
    "val_loader = build_detection_test_loader(cfg, \"CFH_train_average_padded_rotated\", mapper=mapper) \n",
    "outputs = inference_on_dataset(trainer.model, val_loader, evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f06302",
   "metadata": {},
   "source": [
    "# Visualize Model Output and Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f32d758",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in random.sample(test_dict, 3):\n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
    "    v = myVisualizer(im[:, :, ::-1],\n",
    "                  metadata=MetadataCatalog.get(\"Test\"), \n",
    "                  scale=0.5)\n",
    "                  # instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    "    # )\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    plt.imshow(out.get_image()[:, :, ::-1])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
