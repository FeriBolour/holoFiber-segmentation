{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6666945",
   "metadata": {},
   "source": [
    "# Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554be14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import detectron2\n",
    "import contextlib\n",
    "import datetime\n",
    "import io\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import copy,torch,torchvision\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as X\n",
    "import math\n",
    "from itertools import repeat\n",
    "import re\n",
    "import shutil\n",
    "import io\n",
    "import ast\n",
    "\n",
    "from fvcore.common.file_io import PathManager\n",
    "from fvcore.common.timer import Timer\n",
    "\n",
    "from detectron2.structures import Boxes, BoxMode, PolygonMasks\n",
    "from detectron2.config import *\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.data import transforms as T\n",
    "from detectron2.data import detection_utils as utils\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader, build_detection_train_loader\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.engine import DefaultTrainer, DefaultPredictor\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.evaluation import RotatedCOCOEvaluator,DatasetEvaluators, inference_on_dataset, coco_evaluation,DatasetEvaluator\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from platform import python_version\n",
    "\n",
    "import glob\n",
    "import time\n",
    "import shutil\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import concurrent.futures\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "from torch.utils.cpp_extension import CUDA_HOME\n",
    "print(torch.cuda.is_available(), CUDA_HOME)\n",
    "\n",
    "setup_logger()\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "\n",
    "from detectron2.config import configurable\n",
    "from detectron2.layers import ShapeSpec, batched_nms_rotated\n",
    "from detectron2.structures import Instances, RotatedBoxes, pairwise_iou_rotated\n",
    "from detectron2.utils.events import get_event_storage\n",
    "\n",
    "from typing import Dict, List, Optional, Tuple, Any, Iterator, Union\n",
    "import itertools\n",
    "\n",
    "from detectron2.modeling.box_regression import Box2BoxTransformRotated\n",
    "from detectron2.modeling.poolers import ROIPooler\n",
    "from detectron2.modeling.proposal_generator.proposal_utils import add_ground_truth_to_proposals\n",
    "from detectron2.modeling.roi_heads.box_head import build_box_head\n",
    "from detectron2.modeling.roi_heads.mask_head import build_mask_head\n",
    "from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputLayers\n",
    "from detectron2.modeling.roi_heads.roi_heads import ROI_HEADS_REGISTRY, StandardROIHeads, select_foreground_proposals\n",
    "from detectron2.modeling.roi_heads.rotated_fast_rcnn import RotatedFastRCNNOutputLayers\n",
    "from detectron2.utils.colormap import random_color\n",
    "from detectron2.structures.masks import ROIMasks, BitMasks\n",
    "from detectron2.structures import Instances\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "\n",
    "from detectron2.data.detection_utils import convert_image_to_rgb\n",
    "from detectron2.layers import move_device_like\n",
    "from detectron2.structures import ImageList, Instances\n",
    "from detectron2.utils.events import get_event_storage\n",
    "from detectron2.utils.logger import log_first_n\n",
    "\n",
    "from detectron2.evaluation.rotated_coco_evaluation import RotatedCOCOeval\n",
    "from pycocotools import mask as maskUtils\n",
    "\n",
    "from detectron2.modeling.meta_arch.rcnn import META_ARCH_REGISTRY, GeneralizedRCNN\n",
    "import pycocotools.mask as mask_util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102ad9fb",
   "metadata": {},
   "source": [
    "# Custom Function for Preparing Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08c314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rbbox(mask):\n",
    "    import cv2\n",
    "    cnts, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    rbbox = cv2.minAreaRect(cnts[0])\n",
    "    return rbbox\n",
    "\n",
    "\n",
    "\n",
    "def make_rbbox_cotton_dicts(Train_data_path, image_id = 1):\n",
    "\n",
    "    padded_seg_dicts = make_seg_cotton_dicts(Train_data_path)\n",
    "\n",
    "    dataset_list = []\n",
    "    for file in padded_seg_dicts:\n",
    "\n",
    "        img_height = file['height']\n",
    "        img_width = file['width']\n",
    "        img_path = file['file_name']\n",
    "        frame_name = file['fr_name']\n",
    "\n",
    "        dict_holder = {}\n",
    "        dict_holder[\"file_name\"] = img_path\n",
    "        dict_holder[\"height\"] =  img_height\n",
    "        dict_holder[\"width\"] = img_width\n",
    "        dict_holder[\"image_id\"] = image_id\n",
    "        dict_holder[\"fr_name\"] = frame_name\n",
    "\n",
    "        # loop over each instance in current image and save annotations dictionary in a list\n",
    "        annotations = []\n",
    "        for index,variable in enumerate(file['annotations']):\n",
    "            category = variable['category_id']\n",
    "            segment = variable['segmentation']\n",
    "            mymask = detectron2.structures.polygons_to_bitmask(segment, img_height,img_width)\n",
    "            mymask = 255*mymask\n",
    "            rbbox = get_rbbox((mymask).astype('uint8'))\n",
    "            cent_x = rbbox[0][0]\n",
    "            cent_y = rbbox[0][1]\n",
    "            w = rbbox[1][0]\n",
    "            h = rbbox[1][1]\n",
    "            angle = rbbox[2]\n",
    "#             if h > w:\n",
    "#                 angle = 90-angle\n",
    "#             else:\n",
    "            angle = -angle # -angle works best (for now)\n",
    "            bbox = [cent_x, cent_y, w, h, angle]\n",
    "            bbox_mode = detectron2.structures.BoxMode(4) # box_mode = 4 --> (x_cent,y_cent,w,h,a)\n",
    "            dict_annot = {\n",
    "                            \"bbox\": bbox,\n",
    "                            \"bbox_mode\": bbox_mode,\n",
    "                            \"category_id\": category,\n",
    "                        }\n",
    "            annotations.append(dict_annot)\n",
    "\n",
    "        dict_holder[\"annotations\"] = annotations\n",
    "\n",
    "        if 'train' in Train_data_path:\n",
    "                    dataset_list.append(dict_holder)\n",
    "                    image_id += 1\n",
    "        else:\n",
    "            if 'aug' in frame_name:\n",
    "                dataset_list.append(dict_holder)\n",
    "                image_id += 1\n",
    "                \n",
    "    return dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1c790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_data_path = 'train_average'\n",
    "Base_path = 'Cotton Fiber Project'\n",
    "rbbox_train_dicts = make_rbbox_cotton_dicts(Train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93e2ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in [\"train_average\"]: #,,\"val\",\"test\" (enter inside list for val data creation)\n",
    "    DatasetCatalog.register(\"CFH_\" + d,lambda d=d: make_seg_cotton_dicts(os.path.join(Base_path,d)))\n",
    "    MetadataCatalog.get(\"CFH_\" + d).thing_classes=[\"fiber\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1a2253",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_train = MetadataCatalog.get(\"CFH_train_average\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a132d4",
   "metadata": {},
   "source": [
    "# Utilty Functions for Custum ROI-Heads Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38839544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rotated_roi_pixel_mapping(roi):\n",
    "    assert len(roi) == 5  # xc yc w h angle\n",
    "\n",
    "    xc, yc, w, h, angle = roi\n",
    "    \n",
    "    center = (xc, yc)\n",
    "    theta = np.deg2rad(abs(angle))\n",
    "\n",
    "    # paste mask onto image via rotated rect mapping\n",
    "    v_x = (np.cos(theta), np.sin(theta))\n",
    "    v_y = (-np.sin(theta), np.cos(theta))\n",
    "    s_x = center[0] - v_x[0] * ((w - 1) / 2) - v_y[0] * ((h - 1) / 2)\n",
    "    s_y = center[1] - v_x[1] * ((w - 1) / 2) - v_y[1] * ((h - 1) / 2)\n",
    "\n",
    "    M = np.array([[v_x[0], v_y[0], s_x],\n",
    "                  [v_x[1], v_y[1], s_y]])\n",
    "\n",
    "    return M\n",
    "\n",
    "def crop_min_area_rect(image, rect):\n",
    "    \n",
    "    '''\n",
    "    Rotates OpenCV image around center with angle theta (in deg)\n",
    "    then crops the image according to width and height.\n",
    "    Taken from https://stackoverflow.com/questions/11627362/how-to-straighten-a-rotated-rectangle-area-of-an-image-using-opencv-in-python/48553593#48553593\n",
    "    '''\n",
    "    width = rect[2]\n",
    "    height = rect[3]\n",
    "\n",
    "    if np.any(np.isnan(rect)) or np.any(np.isinf(rect)) or width <= 0 or height <= 0:\n",
    "        return np.zeros((0, 0), dtype=image.dtype)\n",
    "\n",
    "    mapping = get_rotated_roi_pixel_mapping(rect)\n",
    "\n",
    "    w = int(np.round(width))\n",
    "    h = int(np.round(height))\n",
    "    return cv2.warpAffine(image, mapping, (w, h), flags=cv2.WARP_INVERSE_MAP)\n",
    "\n",
    "def subimage(image, rect):\n",
    "    \n",
    "    print(rect)\n",
    "    \n",
    "    center = (rect[0], rect[1])\n",
    "    width, height, theta = int(rect[2]), int(rect[3]), abs(rect[4])\n",
    "             \n",
    "    shape = ( image.shape[1], image.shape[0] ) # cv2.warpAffine expects shape in (length, height)\n",
    "\n",
    "    matrix = cv2.getRotationMatrix2D( center=center, angle=theta, scale=1 )\n",
    "    image = cv2.warpAffine( src=image, M=matrix, dsize=shape )\n",
    "\n",
    "    x = int( center[0] - width/2  )\n",
    "    y = int( center[1] - height/2 )\n",
    "    \n",
    "    image = image[ y:y+height, x:x+width ]\n",
    "\n",
    "    return image\n",
    "\n",
    "def polygons_to_bitmask(polygons: List[np.ndarray], height: int, width: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        polygons (list[ndarray]): each array has shape (Nx2,)\n",
    "        height, width (int)\n",
    "    Returns:\n",
    "        ndarray: a bool mask of shape (height, width)\n",
    "    \"\"\"\n",
    "    if len(polygons) == 0:\n",
    "        # COCOAPI does not support empty polygons\n",
    "        return np.zeros((height, width)).astype(np.bool)\n",
    "    rles = mask_util.frPyObjects(polygons, height, width)\n",
    "    rle = mask_util.merge(rles)\n",
    "    return mask_util.decode(rle).astype(np.bool)\n",
    "\n",
    "def rasterize_polygons_within_rotated_box(\n",
    "    polygons: List[np.ndarray], box: np.ndarray, mask_size: int, imHeight, imWidth\n",
    ") -> torch.Tensor:\n",
    "    \n",
    "    assert len(box) == 5\n",
    "    \n",
    "    gt_mask = polygons_to_bitmask(polygons, imHeight, imWidth)\n",
    "    \n",
    "    cropped_mask = crop_min_area_rect(gt_mask.astype(np.uint8), box)\n",
    "    \n",
    "    scaled_mask = cv2.resize(cropped_mask.astype(np.float32), (mask_size, mask_size)) \n",
    "    scaled_mask[scaled_mask < 0.5] = 0\n",
    "    scaled_mask[scaled_mask >= 0.5] = 1\n",
    "    \n",
    "    mask = torch.from_numpy(scaled_mask.astype(bool))\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fdb1bd",
   "metadata": {},
   "source": [
    "## Custom PolygonMasks Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9773a051",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPolygonMasks(PolygonMasks):\n",
    "    \n",
    "    def __init__(self, polygons: List[List[Union[torch.Tensor, np.ndarray]]], imHeight, imWidth):\n",
    "        \n",
    "        super().__init__(polygons)\n",
    "        self.imHeight = imHeight\n",
    "        self.imWidth = imWidth\n",
    "            \n",
    "    def crop_and_resize(self, boxes: torch.Tensor, mask_size: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Crop each mask by the given box, and resize results to (mask_size, mask_size).\n",
    "        This can be used to prepare training targets for Mask R-CNN.\n",
    "        Args:\n",
    "            boxes (Tensor): Nx4 tensor storing the boxes for each mask\n",
    "            mask_size (int): the size of the rasterized mask.\n",
    "        Returns:\n",
    "            Tensor: A bool tensor of shape (N, mask_size, mask_size), where\n",
    "            N is the number of predicted boxes for this image.\n",
    "        \"\"\"\n",
    "        assert len(boxes) == len(self), \"{} != {}\".format(len(boxes), len(self))\n",
    "\n",
    "        device = boxes.device\n",
    "        # Put boxes on the CPU, as the polygon representation is not efficient GPU-wise\n",
    "        # (several small tensors for representing a single instance mask)\n",
    "        boxes = boxes.to(torch.device(\"cpu\"))\n",
    "        \n",
    "        results = [\n",
    "            rasterize_polygons_within_rotated_box(poly, box.numpy(), mask_size, self.imHeight, self.imWidth)\n",
    "            for poly, box in zip(self.polygons, boxes)\n",
    "        ]\n",
    "        \"\"\"\n",
    "        poly: list[list[float]], the polygons for one instance\n",
    "        box: a tensor of shape (4,)\n",
    "        \"\"\"\n",
    "        if len(results) == 0:\n",
    "            return torch.empty(0, mask_size, mask_size, dtype=torch.bool, device=device)\n",
    "        return torch.stack(results, dim=0).to(device=device)\n",
    "    \n",
    "    def to(self, *args: Any, **kwargs: Any) -> \"MyPolygonMasks\":\n",
    "        return self\n",
    "    \n",
    "    def __getitem__(self, item: Union[int, slice, List[int], torch.BoolTensor]) -> \"MyPolygonMasks\":\n",
    "        \"\"\"\n",
    "        Support indexing over the instances and return a `PolygonMasks` object.\n",
    "        `item` can be:\n",
    "        1. An integer. It will return an object with only one instance.\n",
    "        2. A slice. It will return an object with the selected instances.\n",
    "        3. A list[int]. It will return an object with the selected instances,\n",
    "           correpsonding to the indices in the list.\n",
    "        4. A vector mask of type BoolTensor, whose length is num_instances.\n",
    "           It will return an object with the instances whose mask is nonzero.\n",
    "        \"\"\"\n",
    "        if isinstance(item, int):\n",
    "            selected_polygons = [self.polygons[item]]\n",
    "        elif isinstance(item, slice):\n",
    "            selected_polygons = self.polygons[item]\n",
    "        elif isinstance(item, list):\n",
    "            selected_polygons = [self.polygons[i] for i in item]\n",
    "        elif isinstance(item, torch.Tensor):\n",
    "            # Polygons is a list, so we have to move the indices back to CPU.\n",
    "            if item.dtype == torch.bool:\n",
    "                assert item.dim() == 1, item.shape\n",
    "                item = item.nonzero().squeeze(1).cpu().numpy().tolist()\n",
    "            elif item.dtype in [torch.int32, torch.int64]:\n",
    "                item = item.cpu().numpy().tolist()\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported tensor dtype={} for indexing!\".format(item.dtype))\n",
    "            selected_polygons = [self.polygons[i] for i in item]\n",
    "        return MyPolygonMasks(selected_polygons, self.imHeight, self.imWidth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5900ca87",
   "metadata": {},
   "source": [
    "# Custom ROI-Heads Class to support Rotated-BBoxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063c3c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ROI_HEADS_REGISTRY.register()\n",
    "class RROIHeads_wMask(StandardROIHeads):\n",
    "    \n",
    "    @classmethod\n",
    "    def _init_box_head(cls, cfg, input_shape):\n",
    "        # fmt: off\n",
    "        in_features       = cfg.MODEL.ROI_HEADS.IN_FEATURES\n",
    "        pooler_resolution = cfg.MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION\n",
    "        pooler_scales     = tuple(1.0 / input_shape[k].stride for k in in_features)\n",
    "        sampling_ratio    = cfg.MODEL.ROI_BOX_HEAD.POOLER_SAMPLING_RATIO\n",
    "        pooler_type       = cfg.MODEL.ROI_BOX_HEAD.POOLER_TYPE\n",
    "        # fmt: on\n",
    "        assert pooler_type in [\"ROIAlignRotated\"], pooler_type\n",
    "        # assume all channel counts are equal\n",
    "        in_channels = [input_shape[f].channels for f in in_features][0]\n",
    "\n",
    "        box_pooler = ROIPooler(\n",
    "            output_size=pooler_resolution,\n",
    "            scales=pooler_scales,\n",
    "            sampling_ratio=sampling_ratio,\n",
    "            pooler_type=pooler_type,\n",
    "        )\n",
    "        box_head = build_box_head(\n",
    "            cfg, ShapeSpec(channels=in_channels, height=pooler_resolution, width=pooler_resolution)\n",
    "        )\n",
    "        # This line is the only difference v.s. StandardROIHeads\n",
    "        box_predictor = RotatedFastRCNNOutputLayers(cfg, box_head.output_shape)\n",
    "        return {\n",
    "            \"box_in_features\": in_features,\n",
    "            \"box_pooler\": box_pooler,\n",
    "            \"box_head\": box_head,\n",
    "            \"box_predictor\": box_predictor,\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def _init_mask_head(cls, cfg, input_shape):\n",
    "        if not cfg.MODEL.MASK_ON:\n",
    "            return {}\n",
    "        # fmt: off\n",
    "        in_features       = cfg.MODEL.ROI_HEADS.IN_FEATURES\n",
    "        pooler_resolution = cfg.MODEL.ROI_MASK_HEAD.POOLER_RESOLUTION\n",
    "        pooler_scales     = tuple(1.0 / input_shape[k].stride for k in in_features)\n",
    "        sampling_ratio    = cfg.MODEL.ROI_MASK_HEAD.POOLER_SAMPLING_RATIO\n",
    "        pooler_type       = cfg.MODEL.ROI_MASK_HEAD.POOLER_TYPE\n",
    "        # fmt: on\n",
    "        \n",
    "        assert pooler_type in [\"ROIAlignRotated\"], pooler_type\n",
    "\n",
    "        in_channels = [input_shape[f].channels for f in in_features][0]\n",
    "\n",
    "        ret = {\"mask_in_features\": in_features}\n",
    "        ret[\"mask_pooler\"] = (\n",
    "            ROIPooler(\n",
    "                output_size=pooler_resolution,\n",
    "                scales=pooler_scales,\n",
    "                sampling_ratio=sampling_ratio,\n",
    "                pooler_type=pooler_type,\n",
    "            )\n",
    "            if pooler_type\n",
    "            else None\n",
    "        )\n",
    "        if pooler_type:\n",
    "            shape = ShapeSpec(\n",
    "                channels=in_channels, width=pooler_resolution, height=pooler_resolution\n",
    "            )\n",
    "        else:\n",
    "            shape = {f: input_shape[f] for f in in_features}\n",
    "        ret[\"mask_head\"] = build_mask_head(cfg, shape)\n",
    "        return ret\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def label_and_sample_proposals(self, proposals, targets):\n",
    "        \"\"\"\n",
    "        Prepare some proposals to be used to train the RROI heads.\n",
    "        It performs box matching between `proposals` and `targets`, and assigns\n",
    "        training labels to the proposals.\n",
    "        It returns `self.batch_size_per_image` random samples from proposals and groundtruth boxes,\n",
    "        with a fraction of positives that is no larger than `self.positive_sample_fraction.\n",
    "        Args:\n",
    "            See :meth:`StandardROIHeads.forward`\n",
    "        Returns:\n",
    "            list[Instances]: length `N` list of `Instances`s containing the proposals\n",
    "                sampled for training. Each `Instances` has the following fields:\n",
    "                - proposal_boxes: the rotated proposal boxes\n",
    "                - gt_boxes: the ground-truth rotated boxes that the proposal is assigned to\n",
    "                  (this is only meaningful if the proposal has a label > 0; if label = 0\n",
    "                   then the ground-truth box is random)\n",
    "                - gt_classes: the ground-truth classification lable for each proposal\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.proposal_append_gt:\n",
    "            proposals = add_ground_truth_to_proposals(targets, proposals)\n",
    "\n",
    "        proposals_with_gt = []\n",
    "\n",
    "        num_fg_samples = []\n",
    "        num_bg_samples = []\n",
    "        for proposals_per_image, targets_per_image in zip(proposals, targets):\n",
    "            has_gt = len(targets_per_image) > 0\n",
    "            match_quality_matrix = pairwise_iou_rotated(\n",
    "                targets_per_image.gt_boxes, proposals_per_image.proposal_boxes\n",
    "            )\n",
    "            matched_idxs, matched_labels = self.proposal_matcher(match_quality_matrix)\n",
    "            sampled_idxs, gt_classes = self._sample_proposals(\n",
    "                matched_idxs, matched_labels, targets_per_image.gt_classes\n",
    "            )\n",
    "\n",
    "            proposals_per_image = proposals_per_image[sampled_idxs]\n",
    "            proposals_per_image.gt_classes = gt_classes\n",
    "\n",
    "#             if has_gt:\n",
    "#                 sampled_targets = matched_idxs[sampled_idxs]\n",
    "#                 proposals_per_image.gt_boxes = targets_per_image.gt_boxes[sampled_targets]\n",
    "            if has_gt:\n",
    "                sampled_targets = matched_idxs[sampled_idxs]\n",
    "                # We index all the attributes of targets that start with \"gt_\"\n",
    "                # and have not been added to proposals yet (=\"gt_classes\").\n",
    "                # NOTE: here the indexing waste some compute, because heads\n",
    "                # like masks, keypoints, etc, will filter the proposals again,\n",
    "                # (by foreground/background, or number of keypoints in the image, etc)\n",
    "                # so we essentially index the data twice.\n",
    "                for (trg_name, trg_value) in targets_per_image.get_fields().items():\n",
    "                    if trg_name.startswith(\"gt_\") and not proposals_per_image.has(trg_name):\n",
    "                        proposals_per_image.set(trg_name, trg_value[sampled_targets])\n",
    "                        \n",
    "            num_bg_samples.append((gt_classes == self.num_classes).sum().item())\n",
    "            num_fg_samples.append(gt_classes.numel() - num_bg_samples[-1])\n",
    "            proposals_with_gt.append(proposals_per_image)\n",
    "\n",
    "        # Log the number of fg/bg samples that are selected for training ROI heads\n",
    "        storage = get_event_storage()\n",
    "        storage.put_scalar(\"roi_head/num_fg_samples\", np.mean(num_fg_samples))\n",
    "        storage.put_scalar(\"roi_head/num_bg_samples\", np.mean(num_bg_samples))\n",
    "\n",
    "        return proposals_with_gt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d75d90",
   "metadata": {},
   "source": [
    "# Custom Dataset Mapper to support Rotated-BBoxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3f2d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_annotations_to_instances_rotated(annos, image_size, mask_format=\"polygon\"):\n",
    "    \"\"\"\n",
    "    Create an :class:`Instances` object used by the models,\n",
    "    from instance annotations in the dataset dict.\n",
    "    Compared to `annotations_to_instances`, this function is for rotated boxes only\n",
    "\n",
    "    Args:\n",
    "        annos (list[dict]): a list of instance annotations in one image, each\n",
    "            element for one instance.\n",
    "        image_size (tuple): height, width\n",
    "\n",
    "    Returns:\n",
    "        Instances:\n",
    "            Containing fields \"gt_boxes\", \"gt_classes\",\n",
    "            if they can be obtained from `annos`.\n",
    "            This is the format that builtin models expect.\n",
    "    \"\"\"\n",
    "    boxes = [obj[\"bbox\"] for obj in annos]\n",
    "    target = Instances(image_size)\n",
    "    boxes = target.gt_boxes = RotatedBoxes(boxes)\n",
    "    boxes.clip(image_size)\n",
    "\n",
    "    classes = [obj[\"category_id\"] for obj in annos]\n",
    "    classes = torch.tensor(classes, dtype=torch.int64)\n",
    "    target.gt_classes = classes\n",
    "    \n",
    "    if len(annos) and \"segmentation\" in annos[0]:\n",
    "        segms = [obj[\"segmentation\"] for obj in annos]\n",
    "        if mask_format == \"polygon\":\n",
    "            try:\n",
    "                masks = MyPolygonMasks(segms, image_size[0], image_size[1])\n",
    "            except ValueError as e:\n",
    "                raise ValueError(\n",
    "                    \"Failed to use mask_format=='polygon' from the given annotations!\"\n",
    "                ) from e\n",
    "        else:\n",
    "            assert mask_format == \"bitmask\", mask_format\n",
    "            masks = []\n",
    "            for segm in segms:\n",
    "                if isinstance(segm, list):\n",
    "                    # polygon\n",
    "                    masks.append(polygons_to_bitmask(segm, *image_size))\n",
    "                elif isinstance(segm, dict):\n",
    "                    # COCO RLE\n",
    "                    masks.append(mask_util.decode(segm))\n",
    "                elif isinstance(segm, np.ndarray):\n",
    "                    assert segm.ndim == 2, \"Expect segmentation of 2 dimensions, got {}.\".format(\n",
    "                        segm.ndim\n",
    "                    )\n",
    "                    # mask array\n",
    "                    masks.append(segm)\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        \"Cannot convert segmentation of type '{}' to BitMasks!\"\n",
    "                        \"Supported types are: polygons as list[list[float] or ndarray],\"\n",
    "                        \" COCO-style RLE as a dict, or a binary segmentation mask \"\n",
    "                        \" in a 2D numpy array of shape HxW.\".format(type(segm))\n",
    "                    )\n",
    "            # torch.from_numpy does not support array with negative stride.\n",
    "            masks = BitMasks(\n",
    "                torch.stack([torch.from_numpy(np.ascontiguousarray(x)) for x in masks])\n",
    "            )\n",
    "        target.gt_masks = masks\n",
    "        \n",
    "    return target\n",
    "\n",
    "\n",
    "def my_transform_instance_annotations(annotation, transforms, image_size, *, keypoint_hflip_indices=None):\n",
    "    if annotation[\"bbox_mode\"] == BoxMode.XYWHA_ABS:\n",
    "        annotation[\"bbox\"] = transforms.apply_rotated_box(np.asarray([annotation[\"bbox\"]]))[0]\n",
    "    else:\n",
    "        bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n",
    "        # Note that bbox is 1d (per-instance bounding box)\n",
    "        annotation[\"bbox\"] = transforms.apply_box([bbox])[0]\n",
    "        annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n",
    "    \n",
    "    if \"segmentation\" in annotation:\n",
    "        # each instance contains 1 or more polygons\n",
    "        segm = annotation[\"segmentation\"]\n",
    "        if isinstance(segm, list):\n",
    "            # polygons\n",
    "            polygons = [np.asarray(p).reshape(-1, 2) for p in segm]\n",
    "            annotation[\"segmentation\"] = [\n",
    "                p.reshape(-1) for p in transforms.apply_polygons(polygons)\n",
    "            ]\n",
    "        elif isinstance(segm, dict):\n",
    "            # RLE\n",
    "            mask = mask_util.decode(segm)\n",
    "            mask = transforms.apply_segmentation(mask)\n",
    "            assert tuple(mask.shape[:2]) == image_size\n",
    "            annotation[\"segmentation\"] = mask\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Cannot transform segmentation of type '{}'!\"\n",
    "                \"Supported types are: polygons as list[list[float] or ndarray],\"\n",
    "                \" COCO-style RLE as a dict.\".format(type(segm))\n",
    "            )\n",
    "            \n",
    "    return annotation\n",
    "\n",
    "def mapper(dataset_dict):\n",
    "    # Implement a mapper, similar to the default DatasetMapper, but with our own customizations\n",
    "    dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n",
    "    image = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")\n",
    "    image, transforms = T.apply_transform_gens([T.Resize((800, 800))], image)\n",
    "    dataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1).astype(\"float32\"))\n",
    "\n",
    "    annos = [\n",
    "      my_transform_instance_annotations(obj, transforms, image.shape[:2]) \n",
    "      for obj in dataset_dict.pop(\"annotations\")\n",
    "      if obj.get(\"iscrowd\", 0) == 0\n",
    "    ]\n",
    "    instances = my_annotations_to_instances_rotated(annos, image.shape[:2])\n",
    "    dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n",
    "\n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5466acfa",
   "metadata": {},
   "source": [
    "# Custom Trainer and Visualizer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a231b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name):\n",
    "        output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "        evaluators = [RotatedCOCOEvaluator(dataset_name, cfg, True, output_folder)]\n",
    "        return DatasetEvaluators(evaluators)\n",
    "      \n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "        return build_detection_train_loader(cfg, mapper=mapper)\n",
    "\n",
    "class RotatedPredictor(DefaultPredictor):\n",
    "    def __init__(self, cfg):\n",
    "        \n",
    "        self.cfg = cfg.clone()  # cfg can be modified by model\n",
    "        self.model = trainer.model\n",
    "        self.model.eval()\n",
    "\n",
    "        self.transform_gen = T.ResizeShortestEdge(\n",
    "            [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST\n",
    "        )\n",
    "\n",
    "        self.input_format = cfg.INPUT.FORMAT\n",
    "        assert self.input_format in [\"RGB\", \"BGR\"], self.input_format\n",
    "\n",
    "    def __call__(self, original_image):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            original_image (np.ndarray): an image of shape (H, W, C) (in BGR order).\n",
    "        Returns:\n",
    "            predictions (dict):\n",
    "                the output of the model for one image only.\n",
    "                See :doc:`/tutorials/models` for details about the format.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():  # https://github.com/sphinx-doc/sphinx/issues/4258\n",
    "            # Apply pre-processing to image.\n",
    "            if self.input_format == \"RGB\":\n",
    "                # whether the model expects BGR inputs or RGB\n",
    "                original_image = original_image[:, :, ::-1]\n",
    "            height, width = original_image.shape[:2]\n",
    "            image = self.transform_gen.get_transform(original_image).apply_image(original_image)\n",
    "            image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
    "\n",
    "            inputs = {\"image\": image, \"height\": height, \"width\": width}\n",
    "            predictions = self.model([inputs])[0]\n",
    "            return predictions\n",
    "\n",
    "# As of 0.3 the XYWHA_ABS box is not supported in the visualizer, this is fixed in master branch atm (19/11/20)\n",
    "class myVisualizer(Visualizer):\n",
    "  \n",
    "    def draw_dataset_dict(self, dic):\n",
    "        annos = dic.get(\"annotations\", None)\n",
    "        if annos:\n",
    "            if \"segmentation\" in annos[0]:\n",
    "                masks = [x[\"segmentation\"] for x in annos]\n",
    "            else:\n",
    "                masks = None\n",
    "            if \"keypoints\" in annos[0]:\n",
    "                keypts = [x[\"keypoints\"] for x in annos]\n",
    "                keypts = np.array(keypts).reshape(len(annos), -1, 3)\n",
    "            else:\n",
    "                keypts = None\n",
    "\n",
    "            boxes = [BoxMode.convert(x[\"bbox\"], x[\"bbox_mode\"], BoxMode.XYWHA_ABS) for x in annos]\n",
    "\n",
    "            labels = [x[\"category_id\"] for x in annos]\n",
    "            names = self.metadata.get(\"thing_classes\", None)\n",
    "            if names:\n",
    "                labels = [names[i] for i in labels]\n",
    "            labels = [\n",
    "                \"{}\".format(i) + (\"|crowd\" if a.get(\"iscrowd\", 0) else \"\")\n",
    "                for i, a in zip(labels, annos)\n",
    "            ]\n",
    "            self.overlay_instances(labels=labels, boxes=boxes, masks=masks, keypoints=keypts)\n",
    "\n",
    "        sem_seg = dic.get(\"sem_seg\", None)\n",
    "        if sem_seg is None and \"sem_seg_file_name\" in dic:\n",
    "            sem_seg = cv2.imread(dic[\"sem_seg_file_name\"], cv2.IMREAD_GRAYSCALE)\n",
    "        if sem_seg is not None:\n",
    "            self.draw_sem_seg(sem_seg, area_threshold=0, alpha=0.5)\n",
    "        return self.output\n",
    "    \n",
    "    def overlay_instances(\n",
    "        self,\n",
    "        *,\n",
    "        boxes=None,\n",
    "        labels=None,\n",
    "        masks=None,\n",
    "        keypoints=None,\n",
    "        assigned_colors=None,\n",
    "        alpha=0.5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            boxes (Boxes, RotatedBoxes or ndarray): either a :class:`Boxes`,\n",
    "                or an Nx4 numpy array of XYXY_ABS format for the N objects in a single image,\n",
    "                or a :class:`RotatedBoxes`,\n",
    "                or an Nx5 numpy array of (x_center, y_center, width, height, angle_degrees) format\n",
    "                for the N objects in a single image,\n",
    "            labels (list[str]): the text to be displayed for each instance.\n",
    "            masks (masks-like object): Supported types are:\n",
    "\n",
    "                * :class:`detectron2.structures.PolygonMasks`,\n",
    "                  :class:`detectron2.structures.BitMasks`.\n",
    "                * list[list[ndarray]]: contains the segmentation masks for all objects in one image.\n",
    "                  The first level of the list corresponds to individual instances. The second\n",
    "                  level to all the polygon that compose the instance, and the third level\n",
    "                  to the polygon coordinates. The third level should have the format of\n",
    "                  [x0, y0, x1, y1, ..., xn, yn] (n >= 3).\n",
    "                * list[ndarray]: each ndarray is a binary mask of shape (H, W).\n",
    "                * list[dict]: each dict is a COCO-style RLE.\n",
    "            keypoints (Keypoint or array like): an array-like object of shape (N, K, 3),\n",
    "                where the N is the number of instances and K is the number of keypoints.\n",
    "                The last dimension corresponds to (x, y, visibility or score).\n",
    "            assigned_colors (list[matplotlib.colors]): a list of colors, where each color\n",
    "                corresponds to each mask or box in the image. Refer to 'matplotlib.colors'\n",
    "                for full list of formats that the colors are accepted in.\n",
    "        Returns:\n",
    "            output (VisImage): image object with visualizations.\n",
    "        \"\"\"\n",
    "        num_instances = 0\n",
    "        if boxes is not None:\n",
    "            boxes = self._convert_boxes(boxes)\n",
    "            num_instances = len(boxes)\n",
    "        if masks is not None:\n",
    "            masks = self._convert_masks(masks)\n",
    "            if num_instances:\n",
    "                assert len(masks) == num_instances\n",
    "            else:\n",
    "                num_instances = len(masks)\n",
    "        if keypoints is not None:\n",
    "            if num_instances:\n",
    "                assert len(keypoints) == num_instances\n",
    "            else:\n",
    "                num_instances = len(keypoints)\n",
    "            keypoints = self._convert_keypoints(keypoints)\n",
    "        if labels is not None:\n",
    "            assert len(labels) == num_instances\n",
    "        if assigned_colors is None:\n",
    "            assigned_colors = [random_color(rgb=True, maximum=1) for _ in range(num_instances)]\n",
    "        if num_instances == 0:\n",
    "            return self.output\n",
    "        if boxes is not None and boxes.shape[1] == 5:\n",
    "            return self.overlay_rotated_instances(\n",
    "                boxes=boxes, labels=labels, masks=masks, assigned_colors=assigned_colors, alpha=alpha\n",
    "            )\n",
    "\n",
    "        # Display in largest to smallest order to reduce occlusion.\n",
    "        areas = None\n",
    "        if boxes is not None:\n",
    "            areas = np.prod(boxes[:, 2:] - boxes[:, :2], axis=1)\n",
    "        elif masks is not None:\n",
    "            areas = np.asarray([x.area() for x in masks])\n",
    "\n",
    "        if areas is not None:\n",
    "            sorted_idxs = np.argsort(-areas).tolist()\n",
    "            # Re-order overlapped instances in descending order.\n",
    "            boxes = boxes[sorted_idxs] if boxes is not None else None\n",
    "            labels = [labels[k] for k in sorted_idxs] if labels is not None else None\n",
    "            masks = [masks[idx] for idx in sorted_idxs] if masks is not None else None\n",
    "            assigned_colors = [assigned_colors[idx] for idx in sorted_idxs]\n",
    "            keypoints = keypoints[sorted_idxs] if keypoints is not None else None\n",
    "\n",
    "        for i in range(num_instances):\n",
    "            color = assigned_colors[i]\n",
    "            if boxes is not None:\n",
    "                self.draw_box(boxes[i], edge_color=color)\n",
    "\n",
    "            if masks is not None:\n",
    "                for segment in masks[i].polygons:\n",
    "                    self.draw_polygon(segment.reshape(-1, 2), color, alpha=alpha)\n",
    "\n",
    "            if labels is not None:\n",
    "                # first get a box\n",
    "                if boxes is not None:\n",
    "                    x0, y0, x1, y1 = boxes[i]\n",
    "                    text_pos = (x0, y0)  # if drawing boxes, put text on the box corner.\n",
    "                    horiz_align = \"left\"\n",
    "                elif masks is not None:\n",
    "                    # skip small mask without polygon\n",
    "                    if len(masks[i].polygons) == 0:\n",
    "                        continue\n",
    "\n",
    "                    x0, y0, x1, y1 = masks[i].bbox()\n",
    "\n",
    "                    # draw text in the center (defined by median) when box is not drawn\n",
    "                    # median is less sensitive to outliers.\n",
    "                    text_pos = np.median(masks[i].mask.nonzero(), axis=1)[::-1]\n",
    "                    horiz_align = \"center\"\n",
    "                else:\n",
    "                    continue  # drawing the box confidence for keypoints isn't very useful.\n",
    "                # for small objects, draw text at the side to avoid occlusion\n",
    "                instance_area = (y1 - y0) * (x1 - x0)\n",
    "                if (\n",
    "                    instance_area < _SMALL_OBJECT_AREA_THRESH * self.output.scale\n",
    "                    or y1 - y0 < 40 * self.output.scale\n",
    "                ):\n",
    "                    if y1 >= self.output.height - 5:\n",
    "                        text_pos = (x1, y0)\n",
    "                    else:\n",
    "                        text_pos = (x0, y1)\n",
    "\n",
    "                height_ratio = (y1 - y0) / np.sqrt(self.output.height * self.output.width)\n",
    "                lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n",
    "                font_size = (\n",
    "                    np.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2)\n",
    "                    * 0.5\n",
    "                    * self._default_font_size\n",
    "                )\n",
    "                self.draw_text(\n",
    "                    labels[i],\n",
    "                    text_pos,\n",
    "                    color=lighter_color,\n",
    "                    horizontal_alignment=horiz_align,\n",
    "                    font_size=font_size,\n",
    "                )\n",
    "\n",
    "        # draw keypoints\n",
    "        if keypoints is not None:\n",
    "            for keypoints_per_instance in keypoints:\n",
    "                self.draw_and_connect_keypoints(keypoints_per_instance)\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    def overlay_rotated_instances(self, boxes=None, labels=None, masks=None, assigned_colors=None, alpha=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            boxes (ndarray): an Nx5 numpy array of\n",
    "                (x_center, y_center, width, height, angle_degrees) format\n",
    "                for the N objects in a single image.\n",
    "            labels (list[str]): the text to be displayed for each instance.\n",
    "            assigned_colors (list[matplotlib.colors]): a list of colors, where each color\n",
    "                corresponds to each mask or box in the image. Refer to 'matplotlib.colors'\n",
    "                for full list of formats that the colors are accepted in.\n",
    "\n",
    "        Returns:\n",
    "            output (VisImage): image object with visualizations.\n",
    "        \"\"\"\n",
    "        num_instances = len(boxes)\n",
    "\n",
    "        if assigned_colors is None:\n",
    "            assigned_colors = [random_color(rgb=True, maximum=1) for _ in range(num_instances)]\n",
    "        if num_instances == 0:\n",
    "            return self.output\n",
    "\n",
    "        # Display in largest to smallest order to reduce occlusion.\n",
    "        if boxes is not None:\n",
    "            areas = boxes[:, 2] * boxes[:, 3]\n",
    "\n",
    "        sorted_idxs = np.argsort(-areas).tolist()\n",
    "        # Re-order overlapped instances in descending order.\n",
    "        boxes = boxes[sorted_idxs]\n",
    "        labels = [labels[k] for k in sorted_idxs] if labels is not None else None\n",
    "        colors = [assigned_colors[idx] for idx in sorted_idxs]\n",
    "        masks = [masks[idx] for idx in sorted_idxs] if masks is not None else None\n",
    "\n",
    "        for i in range(num_instances):\n",
    "            self.draw_rotated_box_with_label(\n",
    "                boxes[i], edge_color=colors[i], label=labels[i] if labels is not None else None\n",
    "            )\n",
    "            if masks is not None:\n",
    "                for segment in masks[i].polygons:\n",
    "                    self.draw_polygon(segment.reshape(-1, 2), color=colors[i], alpha=alpha)\n",
    "\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25a4ad2",
   "metadata": {},
   "source": [
    "## Function to Save the Detectron2 Config into Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b8fbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cfg2yaml(cfg):\n",
    "    \n",
    "    with open(cfg.OUTPUT_DIR + \"/Config.txt\", 'w') as file:\n",
    "        file.write(cfg.dump())\n",
    "    \n",
    "    os.rename(cfg.OUTPUT_DIR + \"/Config.txt\", cfg.OUTPUT_DIR + \"/Config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acbdb95",
   "metadata": {},
   "source": [
    "# Setup Detectron2's Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6eefab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "\n",
    "cfg.OUTPUT_DIR = 'MaskRCNN Test'\n",
    "\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "\n",
    "cfg.DATASETS.TRAIN = (\"CFH_train_average_padded_rotated\",)\n",
    "cfg.DATASETS.TEST = ()\n",
    "\n",
    "cfg.MODEL.MASK_ON=True\n",
    "cfg.MODEL.PROPOSAL_GENERATOR.NAME = \"RRPN\"\n",
    "cfg.MODEL.RPN.HEAD_NAME = \"StandardRPNHead\"\n",
    "cfg.MODEL.RPN.BBOX_REG_WEIGHTS = (10,10,5,5,1)\n",
    "cfg.MODEL.ANCHOR_GENERATOR.NAME = \"RotatedAnchorGenerator\"\n",
    "cfg.MODEL.ANCHOR_GENERATOR.ANGLES = [[-90,-60,-30,0,30,60,90]]\n",
    "# cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.9 \n",
    "cfg.MODEL.ROI_HEADS.NAME = \"RROIHeads_wMask\"\n",
    "# cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512   #this is far lower than usual.  \n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES =1\n",
    "cfg.MODEL.ROI_BOX_HEAD.POOLER_TYPE = \"ROIAlignRotated\"\n",
    "cfg.MODEL.ROI_MASK_HEAD.POOLER_TYPE = \"ROIAlignRotated\"\n",
    "cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS = (10,10,5,5,1)\n",
    "# cfg.MODEL.ROI_BOX_HEAD.NUM_CONV=4\n",
    "# cfg.MODEL.ROI_MASK_HEAD.NUM_CONV=8\n",
    "cfg.SOLVER.IMS_PER_BATCH = 14 #can be up to  24 for a p100 (6 default)\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD=2500\n",
    "cfg.SOLVER.BASE_LR = 0.0015\n",
    "# cfg.SOLVER.GAMMA=0.5\n",
    "cfg.SOLVER.STEPS=(12500, 14000)\n",
    "cfg.SOLVER.MAX_ITER=15000\n",
    "\n",
    "cfg.DATALOADER.NUM_WORKERS = 1\n",
    "cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS = True \n",
    "# cfg.DATALOADER.SAMPLER_TRAIN= \"RepeatFactorTrainingSampler\"\n",
    "# cfg.DATALOADER.REPEAT_THRESHOLD=0.01\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)#lets just check our output dir exists\n",
    "# cfg.MODEL.BACKBONE.FREEZE_AT=6\n",
    "cfg2yaml(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e824a425",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MyTrainer(cfg) \n",
    "trainer.resume_or_load(resume=True)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800553c3",
   "metadata": {},
   "source": [
    "## Display Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c5cf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "plt.rcParams['figure.figsize'] = [14, 7]\n",
    "def load_json_arr(json_path):\n",
    "    lines = []\n",
    "    with open(json_path, 'r') as f:\n",
    "        for line in f:\n",
    "            lines.append(json.loads(line))\n",
    "    return lines\n",
    "\n",
    "experiment_metrics = load_json_arr(cfg.OUTPUT_DIR + '/metrics.json')\n",
    "plt.grid(True, which=\"both\")\n",
    "plt.semilogy(\n",
    "    [x['iteration'] for x in experiment_metrics if 'total_loss' in x], \n",
    "    [x['total_loss'] for x in experiment_metrics if 'total_loss' in x])\n",
    "plt.legend(['training_loss'])\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Total Loss')\n",
    "plt.savefig(cfg.OUTPUT_DIR +  '/Loss Curve.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6ead23",
   "metadata": {},
   "source": [
    "## Display Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4beb257",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.grid(True, which=\"both\")\n",
    "plt.plot(\n",
    "    [x['iteration'] for x in experiment_metrics if 'mask_rcnn/accuracy' in x], \n",
    "    [x['mask_rcnn/accuracy'] for x in experiment_metrics if 'mask_rcnn/accuracy' in x])\n",
    "plt.legend(['mask_rcnn_accuracy'], loc='upper left')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Mask R-CNN Accuracy')\n",
    "plt.savefig(cfg.OUTPUT_DIR +  '/mask_rcnn_accuracy_Curve.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09edf6ed",
   "metadata": {},
   "source": [
    "# Custom Classes and Functions for Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff04277",
   "metadata": {},
   "source": [
    "## Utiliy Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc491f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "BYTES_PER_FLOAT = 4\n",
    "# TODO: This memory limit may be too much or too little. It would be better to\n",
    "# determine it based on available resources.\n",
    "GPU_MEM_LIMIT = 1024**3  # 1 GB memory limit\n",
    "\n",
    "def expand_boxes(boxes, scale):\n",
    "    w_half = (boxes[:, 2] - boxes[:, 0]) * .5\n",
    "    h_half = (boxes[:, 3] - boxes[:, 1]) * .5\n",
    "    x_c = (boxes[:, 2] + boxes[:, 0]) * .5\n",
    "    y_c = (boxes[:, 3] + boxes[:, 1]) * .5\n",
    "\n",
    "    w_half *= scale\n",
    "    h_half *= scale\n",
    "\n",
    "    boxes_exp = torch.zeros_like(boxes)\n",
    "    boxes_exp[:, 0] = x_c - w_half\n",
    "    boxes_exp[:, 2] = x_c + w_half\n",
    "    boxes_exp[:, 1] = y_c - h_half\n",
    "    boxes_exp[:, 3] = y_c + h_half\n",
    "    return boxes_exp\n",
    "\n",
    "\n",
    "def expand_masks(mask, padding):\n",
    "    N = mask.shape[0]\n",
    "    M = mask.shape[-1]\n",
    "    pad2 = 2 * padding\n",
    "    scale = float(M + pad2) / M\n",
    "    padded_mask = mask.new_zeros((N, 1, M + pad2, M + pad2))\n",
    "\n",
    "    padded_mask[:, :, padding:-padding, padding:-padding] = mask\n",
    "    return padded_mask, scale\n",
    "\n",
    "def interpolate(\n",
    "    input, size=None, scale_factor=None, mode=\"nearest\", align_corners=None\n",
    "):\n",
    "    if input.numel() > 0:\n",
    "        return torch.nn.functional.interpolate(\n",
    "            input, size, scale_factor, mode, align_corners\n",
    "        )\n",
    "\n",
    "    def _check_size_scale_factor(dim):\n",
    "        if size is None and scale_factor is None:\n",
    "            raise ValueError(\"either size or scale_factor should be defined\")\n",
    "        if size is not None and scale_factor is not None:\n",
    "            raise ValueError(\"only one of size or scale_factor should be defined\")\n",
    "        if (\n",
    "            scale_factor is not None\n",
    "            and isinstance(scale_factor, tuple)\n",
    "            and len(scale_factor) != dim\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"scale_factor shape must match input shape. \"\n",
    "                \"Input is {}D, scale_factor size is {}\".format(dim, len(scale_factor))\n",
    "            )\n",
    "\n",
    "    def _output_size(dim):\n",
    "        _check_size_scale_factor(dim)\n",
    "        if size is not None:\n",
    "            return size\n",
    "        scale_factors = _ntuple(dim)(scale_factor)\n",
    "        # math.floor might return float in py2.7\n",
    "        return [\n",
    "            int(math.floor(input.size(i + 2) * scale_factors[i])) for i in range(dim)\n",
    "        ]\n",
    "\n",
    "    output_shape = tuple(_output_size(2))\n",
    "    output_shape = input.shape[:-2] + output_shape\n",
    "    return _NewEmptyTensorOp.apply(input, output_shape)\n",
    "\n",
    "def paste_rotated_roi_in_image(image, roi_image, roi):\n",
    "    assert len(roi) == 5  # xc yc w h angle\n",
    "\n",
    "    w = roi[2]\n",
    "    h = roi[3]\n",
    "\n",
    "    w = int(np.round(w))\n",
    "    h = int(np.round(h))\n",
    "    rh, rw = roi_image.shape[:2]\n",
    "    if rw != w or rh != h:\n",
    "        roi_image = cv2.resize(roi_image, (w, h))\n",
    "\n",
    "    # generate the mapping of points from roi_image to an image\n",
    "    roi[4] = -roi[4]\n",
    "    M = get_rotated_roi_pixel_mapping(roi)\n",
    "\n",
    "    x_grid, y_grid = np.meshgrid(np.arange(w), np.arange(h))\n",
    "    x_grid = x_grid.reshape(-1)\n",
    "    y_grid = y_grid.reshape(-1)\n",
    "    map_pts_x = x_grid * M[0, 0] + y_grid * M[0, 1] + M[0, 2]\n",
    "    map_pts_y = x_grid * M[1, 0] + y_grid * M[1, 1] + M[1, 2]\n",
    "    map_pts_x = np.round(map_pts_x).astype(np.int32)\n",
    "    map_pts_y = np.round(map_pts_y).astype(np.int32)\n",
    "\n",
    "    # stick onto image\n",
    "    im_h, im_w = image.shape[:2]\n",
    "\n",
    "    valid_x = np.logical_and(map_pts_x >= 0, map_pts_x < im_w)\n",
    "    valid_y = np.logical_and(map_pts_y >= 0, map_pts_y < im_h)\n",
    "    valid = np.logical_and(valid_x, valid_y)\n",
    "    image[map_pts_y[valid], map_pts_x[valid]] = roi_image[y_grid[valid], x_grid[valid]]\n",
    "\n",
    "    # close holes that arise due to rounding from the pixel mapping phase\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    image = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    return image\n",
    "\n",
    "def my_do_paste_mask(mask, box, im_h, im_w, thresh=0.2, padding=1):\n",
    "    # Need to work on the CPU, where fp16 isn't supported - cast to float to avoid this\n",
    "    mask = mask[0].float().cpu()\n",
    "    box = box[0].float().cpu()\n",
    "    \n",
    "    sumH = np.sum(np.sum(mask.detach().numpy().reshape((28,28)), axis = 1) < 0.01)\n",
    "    sumW = np.sum(np.sum(mask.detach().numpy().reshape((28,28)), axis = 0) < 0.01)\n",
    "    \n",
    "    \n",
    "    assert len(box) == 5  # xc yc w h angle\n",
    "\n",
    "    w = int(np.round(box[2]))\n",
    "    h = int(np.round(box[3]))\n",
    "    w = max(w, 1)\n",
    "    h = max(h, 1)\n",
    "    \n",
    "    if (((sumH < sumW) and (h < w)) or ((sumH > sumW) and (h > w))): #and abs(box[4]) <= 45:\n",
    "        mask = torch.tensor(cv2.rotate(mask.detach().numpy().reshape((28,28)), cv2.ROTATE_90_CLOCKWISE))\n",
    "    \n",
    "    # Set shape to [batchxCxHxW]\n",
    "    mask = mask.expand((1, 1, -1, -1))\n",
    "    \n",
    "    # Resize mask\n",
    "    mask = mask.to(torch.float32)\n",
    "    mask = interpolate(mask, size=(h, w), mode='bilinear', align_corners=False)\n",
    "    \n",
    "    mask = mask[0][0]\n",
    "    \n",
    "    if thresh >= 0:\n",
    "        mask = (mask > thresh).to(dtype=torch.float32)\n",
    "\n",
    "    im_mask = np.zeros((im_h, im_w), dtype=np.float32)\n",
    "    im_mask = paste_rotated_roi_in_image(im_mask, mask.cpu().numpy(), box)\n",
    "    \n",
    "    return torch.from_numpy(im_mask).to('cuda:0'), ()\n",
    "\n",
    "# Annotate boxes as Tensor (but not Boxes) in order to use scripting\n",
    "@torch.jit.script_if_tracing\n",
    "def paste_masks_in_image_rotated(\n",
    "    masks: torch.Tensor, boxes: torch.Tensor, image_shape: Tuple[int, int], threshold: float = 0.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Paste a set of masks that are of a fixed resolution (e.g., 28 x 28) into an image.\n",
    "    The location, height, and width for pasting each mask is determined by their\n",
    "    corresponding bounding boxes in boxes.\n",
    "    Note:\n",
    "        This is a complicated but more accurate implementation. In actual deployment, it is\n",
    "        often enough to use a faster but less accurate implementation.\n",
    "        See :func:`paste_mask_in_image_old` in this file for an alternative implementation.\n",
    "    Args:\n",
    "        masks (tensor): Tensor of shape (Bimg, Hmask, Wmask), where Bimg is the number of\n",
    "            detected object instances in the image and Hmask, Wmask are the mask width and mask\n",
    "            height of the predicted mask (e.g., Hmask = Wmask = 28). Values are in [0, 1].\n",
    "        boxes (Boxes or Tensor): A Boxes of length Bimg or Tensor of shape (Bimg, 4).\n",
    "            boxes[i] and masks[i] correspond to the same object instance.\n",
    "        image_shape (tuple): height, width\n",
    "        threshold (float): A threshold in [0, 1] for converting the (soft) masks to\n",
    "            binary masks.\n",
    "    Returns:\n",
    "        img_masks (Tensor): A tensor of shape (Bimg, Himage, Wimage), where Bimg is the\n",
    "        number of detected object instances and Himage, Wimage are the image width\n",
    "        and height. img_masks[i] is a binary mask for object instance i.\n",
    "    \"\"\"\n",
    "\n",
    "    assert masks.shape[-1] == masks.shape[-2], \"Only square mask predictions are supported\"\n",
    "    N = len(masks)\n",
    "    if N == 0:\n",
    "        return masks.new_empty((0,) + image_shape, dtype=torch.uint8)\n",
    "    if not isinstance(boxes, torch.Tensor):\n",
    "        boxes = boxes.tensor\n",
    "    device = boxes.device\n",
    "    assert len(boxes) == N, boxes.shape\n",
    "\n",
    "    img_h, img_w = image_shape\n",
    "\n",
    "    # The actual implementation split the input into chunks,\n",
    "    # and paste them chunk by chunk.\n",
    "#     if device.type == \"cpu\" or torch.jit.is_scripting():\n",
    "#         # CPU is most efficient when they are pasted one by one with skip_empty=True\n",
    "#         # so that it performs minimal number of operations.\n",
    "#         num_chunks = N\n",
    "#     else:\n",
    "#         # GPU benefits from parallelism for larger chunks, but may have memory issue\n",
    "#         # int(img_h) because shape may be tensors in tracing\n",
    "#         num_chunks = int(np.ceil(N * int(img_h) * int(img_w) * BYTES_PER_FLOAT / GPU_MEM_LIMIT))\n",
    "#         assert (\n",
    "#             num_chunks <= N\n",
    "#         ), \"Default GPU_MEM_LIMIT in mask_ops.py is too small; try increasing it\"\n",
    "    \n",
    "    num_chunks = N\n",
    "    chunks = torch.chunk(torch.arange(N, device=device), num_chunks)\n",
    "\n",
    "    img_masks = torch.zeros(\n",
    "        N, img_h, img_w, device=device, dtype=torch.bool if threshold >= 0 else torch.uint8\n",
    "    )\n",
    "    for inds in chunks:\n",
    "        masks_chunk, spatial_inds = my_do_paste_mask(\n",
    "            masks[inds, None, :, :], boxes[inds], img_h, img_w\n",
    "        )\n",
    "\n",
    "        if threshold >= 0:\n",
    "            masks_chunk = (masks_chunk >= threshold).to(dtype=torch.bool)\n",
    "        else:\n",
    "            # for visualization and debugging\n",
    "            masks_chunk = (masks_chunk * 255).to(dtype=torch.uint8)\n",
    "\n",
    "        if torch.jit.is_scripting():  # Scripting does not use the optimized codepath\n",
    "            img_masks[inds] = masks_chunk\n",
    "        else:\n",
    "            img_masks[(inds,) + spatial_inds] = masks_chunk\n",
    "    return img_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbb207d",
   "metadata": {},
   "source": [
    "## Custom ROI-Masks class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f90104",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyROIMasks(ROIMasks):\n",
    "    \n",
    "    @torch.jit.unused\n",
    "    def to_bitmasks(self, boxes: torch.Tensor, height, width, threshold=0.5):\n",
    "        \n",
    "        bitmasks = paste_masks_in_image_rotated(self.tensor, boxes.tensor, (height, width), threshold=threshold)\n",
    "        \n",
    "        return BitMasks(bitmasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca9bfce",
   "metadata": {},
   "source": [
    "## Function to postprocess the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752bcb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_detector_postprocess(\n",
    "    results: Instances, output_height: int, output_width: int, mask_threshold: float = 0.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Resize the output instances.\n",
    "    The input images are often resized when entering an object detector.\n",
    "    As a result, we often need the outputs of the detector in a different\n",
    "    resolution from its inputs.\n",
    "    This function will resize the raw outputs of an R-CNN detector\n",
    "    to produce outputs according to the desired output resolution.\n",
    "    Args:\n",
    "        results (Instances): the raw outputs from the detector.\n",
    "            `results.image_size` contains the input image resolution the detector sees.\n",
    "            This object might be modified in-place.\n",
    "        output_height, output_width: the desired output resolution.\n",
    "    Returns:\n",
    "        Instances: the resized output from the model, based on the output resolution\n",
    "    \"\"\"\n",
    "    if isinstance(output_width, torch.Tensor):\n",
    "        # This shape might (but not necessarily) be tensors during tracing.\n",
    "        # Converts integer tensors to float temporaries to ensure true\n",
    "        # division is performed when computing scale_x and scale_y.\n",
    "        output_width_tmp = output_width.float()\n",
    "        output_height_tmp = output_height.float()\n",
    "        new_size = torch.stack([output_height, output_width])\n",
    "    else:\n",
    "        new_size = (output_height, output_width)\n",
    "        output_width_tmp = output_width\n",
    "        output_height_tmp = output_height\n",
    "\n",
    "    scale_x, scale_y = (\n",
    "        output_width_tmp / results.image_size[1],\n",
    "        output_height_tmp / results.image_size[0],\n",
    "    )\n",
    "    results = Instances(new_size, **results.get_fields())\n",
    "\n",
    "    if results.has(\"pred_boxes\"):\n",
    "        output_boxes = results.pred_boxes\n",
    "    elif results.has(\"proposal_boxes\"):\n",
    "        output_boxes = results.proposal_boxes\n",
    "    else:\n",
    "        output_boxes = None\n",
    "    assert output_boxes is not None, \"Predictions must contain boxes!\"\n",
    "\n",
    "    output_boxes.scale(scale_x, scale_y)\n",
    "    output_boxes.clip(results.image_size)\n",
    "\n",
    "    results = results[output_boxes.nonempty()]\n",
    "\n",
    "    if results.has(\"pred_masks\"):\n",
    "        if isinstance(results.pred_masks, ROIMasks):\n",
    "            roi_masks = results.pred_masks\n",
    "        else:\n",
    "            # pred_masks is a tensor of shape (N, 1, M, M)\n",
    "            roi_masks = MyROIMasks(results.pred_masks[:, 0, :, :])\n",
    "        results.pred_masks = roi_masks.to_bitmasks(\n",
    "            results.pred_boxes, output_height, output_width, mask_threshold\n",
    "        ).tensor  # TODO return ROIMasks/BitMask object in the future\n",
    "\n",
    "    if results.has(\"pred_keypoints\"):\n",
    "        results.pred_keypoints[:, :, 0] *= scale_x\n",
    "        results.pred_keypoints[:, :, 1] *= scale_y\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490bc80b",
   "metadata": {},
   "source": [
    "# Custom GeneralizedRCNN class for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660348f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@META_ARCH_REGISTRY.register()\n",
    "class MyGeneralizedRCNN(GeneralizedRCNN):\n",
    "    \n",
    "    def inference(\n",
    "        self,\n",
    "        batched_inputs: List[Dict[str, torch.Tensor]],\n",
    "        detected_instances: Optional[List[Instances]] = None,\n",
    "        do_postprocess: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Run inference on the given inputs.\n",
    "        Args:\n",
    "            batched_inputs (list[dict]): same as in :meth:`forward`\n",
    "            detected_instances (None or list[Instances]): if not None, it\n",
    "                contains an `Instances` object per image. The `Instances`\n",
    "                object contains \"pred_boxes\" and \"pred_classes\" which are\n",
    "                known boxes in the image.\n",
    "                The inference will then skip the detection of bounding boxes,\n",
    "                and only predict other per-ROI outputs.\n",
    "            do_postprocess (bool): whether to apply post-processing on the outputs.\n",
    "        Returns:\n",
    "            When do_postprocess=True, same as in :meth:`forward`.\n",
    "            Otherwise, a list[Instances] containing raw network outputs.\n",
    "        \"\"\"\n",
    "        assert not self.training\n",
    "\n",
    "        images = self.preprocess_image(batched_inputs)\n",
    "        features = self.backbone(images.tensor)\n",
    "\n",
    "        if detected_instances is None:\n",
    "            if self.proposal_generator is not None:\n",
    "                proposals, _ = self.proposal_generator(images, features, None)\n",
    "            else:\n",
    "                assert \"proposals\" in batched_inputs[0]\n",
    "                proposals = [x[\"proposals\"].to(self.device) for x in batched_inputs]\n",
    "\n",
    "            results, _ = self.roi_heads(images, features, proposals, None)\n",
    "        else:\n",
    "            detected_instances = [x.to(self.device) for x in detected_instances]\n",
    "            results = self.roi_heads.forward_with_given_boxes(features, detected_instances)\n",
    "\n",
    "        if do_postprocess:\n",
    "            assert not torch.jit.is_scripting(), \"Scripting is not supported for postprocess.\"\n",
    "            return MyGeneralizedRCNN._postprocess(results, batched_inputs, images.image_sizes)\n",
    "        return results\n",
    "    \n",
    "    @staticmethod\n",
    "    def _postprocess(instances, batched_inputs: List[Dict[str, torch.Tensor]], image_sizes):\n",
    "        \"\"\"\n",
    "        Rescale the output instances to the target size.\n",
    "        \"\"\"\n",
    "        # note: private function; subject to changes\n",
    "        processed_results = []\n",
    "        for results_per_image, input_per_image, image_size in zip(\n",
    "            instances, batched_inputs, image_sizes\n",
    "        ):\n",
    "            height = input_per_image.get(\"height\", image_size[0])\n",
    "            width = input_per_image.get(\"width\", image_size[1])\n",
    "            r = my_detector_postprocess(results_per_image, height, width)\n",
    "            processed_results.append({\"instances\": r})\n",
    "        return processed_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4643ffc3",
   "metadata": {},
   "source": [
    "# Custom COCO Evaluator Class\n",
    "\n",
    "The following classes and funtions are still under development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d944e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRotatedCOCOeval(RotatedCOCOeval):\n",
    "    \n",
    "    def computeIoU(self, imgId, catId):\n",
    "        p = self.params\n",
    "        if p.useCats:\n",
    "            gt = self._gts[imgId, catId]\n",
    "            dt = self._dts[imgId, catId]\n",
    "        else:\n",
    "            gt = [_ for cId in p.catIds for _ in self._gts[imgId, cId]]\n",
    "            dt = [_ for cId in p.catIds for _ in self._dts[imgId, cId]]\n",
    "        if len(gt) == 0 and len(dt) == 0:\n",
    "            return []\n",
    "        inds = np.argsort([-d[\"score\"] for d in dt], kind=\"mergesort\")\n",
    "        dt = [dt[i] for i in inds]\n",
    "        if len(dt) > p.maxDets[-1]:\n",
    "            dt = dt[0 : p.maxDets[-1]]\n",
    "\n",
    "        if p.iouType == 'segm':\n",
    "            g = [g['segmentation'] for g in gt]\n",
    "            d = [d['segmentation'] for d in dt]\n",
    "        elif p.iouType == 'bbox':\n",
    "            g = [g['bbox'] for g in gt]\n",
    "            d = [d['bbox'] for d in dt]\n",
    "        else:\n",
    "            raise Exception('unknown iouType for iou computation')\n",
    "\n",
    "        # compute iou between each dt and gt region\n",
    "        iscrowd = [int(o[\"iscrowd\"]) for o in gt]\n",
    "\n",
    "        # Note: this function is copied from cocoeval.py in cocoapi\n",
    "        # and the major difference is here.\n",
    "        if p.iouType == 'segm':\n",
    "            ious = maskUtils.iou(d,g,iscrowd)\n",
    "        elif p.iouType == 'bbox':\n",
    "            ious = self.compute_iou_dt_gt(d, g, iscrowd)\n",
    "        return ious\n",
    "    \n",
    "# Bug in RotatedCOCOEvaluator where it gets passed img_ids\n",
    "class MyRotatedCOCOEvaluator(RotatedCOCOEvaluator):              \n",
    "                \n",
    "    def instances_to_json(self, instances, img_id):\n",
    "        num_instance = len(instances)\n",
    "        if num_instance == 0:\n",
    "            return []\n",
    "\n",
    "        boxes = instances.pred_boxes.tensor.numpy()\n",
    "        if boxes.shape[1] == 4:\n",
    "            boxes = BoxMode.convert(boxes, BoxMode.XYXY_ABS, BoxMode.XYWH_ABS)\n",
    "        boxes = boxes.tolist()\n",
    "        scores = instances.scores.tolist()\n",
    "        classes = instances.pred_classes.tolist()\n",
    "        \n",
    "        has_mask = instances.has(\"pred_masks\")\n",
    "        if has_mask:\n",
    "            # use RLE to encode the masks, because they are too large and takes memory\n",
    "            # since this evaluator stores outputs of the entire dataset\n",
    "            rles = [\n",
    "                mask_util.encode(np.array(mask[:, :, None], order=\"F\", dtype=\"uint8\"))[0]\n",
    "                for mask in instances.pred_masks\n",
    "            ]\n",
    "            for rle in rles:\n",
    "                # \"counts\" is an array encoded by mask_util as a byte-stream. Python3's\n",
    "                # json writer which always produces strings cannot serialize a bytestream\n",
    "                # unless you decode it. Thankfully, utf-8 works out (which is also what\n",
    "                # the pycocotools/_mask.pyx does).\n",
    "                rle[\"counts\"] = rle[\"counts\"].decode(\"utf-8\")\n",
    "            \n",
    "        results = []\n",
    "        for k in range(num_instance):\n",
    "            result = {\n",
    "                \"image_id\": img_id,\n",
    "                \"category_id\": classes[k],\n",
    "                \"bbox\": boxes[k],\n",
    "                \"score\": scores[k],\n",
    "            }\n",
    "            if has_mask:\n",
    "                result[\"segmentation\"] = rles[k]\n",
    "            \n",
    "            results.append(result)\n",
    "        return results\n",
    "    \n",
    "    def _eval_predictions(self, predictions, img_ids=None):  # img_ids: unused\n",
    "        \"\"\"\n",
    "        Evaluate predictions on the given tasks.\n",
    "        Fill self._results with the metrics of the tasks.\n",
    "        \"\"\"\n",
    "        self._logger.info(\"Preparing results for COCO format ...\")\n",
    "        coco_results = list(itertools.chain(*[x[\"instances\"] for x in predictions]))\n",
    "        tasks = self._tasks or self._tasks_from_predictions(coco_results)\n",
    "        \n",
    "        # unmap the category ids for COCO\n",
    "        if hasattr(self._metadata, \"thing_dataset_id_to_contiguous_id\"):\n",
    "            dataset_id_to_contiguous_id = self._metadata.thing_dataset_id_to_contiguous_id\n",
    "            all_contiguous_ids = list(dataset_id_to_contiguous_id.values())\n",
    "            num_classes = len(all_contiguous_ids)\n",
    "            assert min(all_contiguous_ids) == 0 and max(all_contiguous_ids) == num_classes - 1\n",
    "\n",
    "            reverse_id_mapping = {v: k for k, v in dataset_id_to_contiguous_id.items()}\n",
    "            for result in coco_results:\n",
    "                category_id = result[\"category_id\"]\n",
    "                assert category_id < num_classes, (\n",
    "                    f\"A prediction has class={category_id}, \"\n",
    "                    f\"but the dataset only has {num_classes} classes and \"\n",
    "                    f\"predicted class id should be in [0, {num_classes - 1}].\"\n",
    "                )\n",
    "                result[\"category_id\"] = reverse_id_mapping[category_id]\n",
    "\n",
    "        if self._output_dir:\n",
    "            file_path = os.path.join(self._output_dir, \"coco_instances_results.json\")\n",
    "            self._logger.info(\"Saving results to {}\".format(file_path))\n",
    "            with PathManager.open(file_path, \"w\") as f:\n",
    "                f.write(json.dumps(coco_results))\n",
    "                f.flush()\n",
    "\n",
    "        if not self._do_evaluation:\n",
    "            self._logger.info(\"Annotations are not available for evaluation.\")\n",
    "            return\n",
    "\n",
    "        self._logger.info(\"Evaluating predictions ...\")\n",
    "\n",
    "#         assert self._tasks is None or set(self._tasks) == {\n",
    "#             \"bbox\"\n",
    "#         }, \"[RotatedCOCOEvaluator] Only bbox evaluation is supported\"\n",
    "#         coco_eval = (\n",
    "#             self._evaluate_predictions_on_coco(self._coco_api, coco_results)\n",
    "#             if len(coco_results) > 0\n",
    "#             else None  # cocoapi does not handle empty results very well\n",
    "#         )\n",
    "        \n",
    "        for task in sorted(tasks):\n",
    "            assert task in {\"bbox\", \"segm\", \"keypoints\"}, f\"Got unknown task: {task}!\"\n",
    "            coco_eval = (\n",
    "                self._evaluate_predictions_on_coco(\n",
    "                    self._coco_api,\n",
    "                    coco_results,\n",
    "                    task,\n",
    "                    img_ids=img_ids\n",
    "                )\n",
    "                if len(coco_results) > 0\n",
    "                else None  # cocoapi does not handle empty results very well\n",
    "            )\n",
    "\n",
    "        res = self._derive_coco_results(\n",
    "                coco_eval, task, class_names=self._metadata.get(\"thing_classes\")\n",
    "            )\n",
    "        self._results[task] = res\n",
    "            \n",
    "    def _evaluate_predictions_on_coco(self, coco_gt, coco_results, iou_type, img_ids=None):\n",
    "        \"\"\"\n",
    "        Evaluate the coco results using COCOEval API.\n",
    "        \"\"\"\n",
    "        assert len(coco_results) > 0\n",
    "        \n",
    "        if iou_type == \"segm\":\n",
    "            coco_results = copy.deepcopy(coco_results)\n",
    "            # When evaluating mask AP, if the results contain bbox, cocoapi will\n",
    "            # use the box area as the area of the instance, instead of the mask area.\n",
    "            # This leads to a different definition of small/medium/large.\n",
    "            # We remove the bbox field to let mask AP use mask area.\n",
    "            for c in coco_results:\n",
    "                c.pop(\"bbox\", None)\n",
    "            \n",
    "        coco_dt = coco_gt.loadRes(coco_results)\n",
    "        \n",
    "        if img_ids is not None:\n",
    "            coco_eval.params.imgIds = img_ids\n",
    "        \n",
    "        # Only bbox is supported for now\n",
    "        coco_eval = RotatedCOCOeval(coco_gt, coco_dt, iouType=\"bbox\")\n",
    "\n",
    "        coco_eval.evaluate()\n",
    "        coco_eval.accumulate()\n",
    "        coco_eval.summarize()\n",
    "\n",
    "        return coco_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb538c13",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75be5d7e",
   "metadata": {},
   "source": [
    "## Setup config for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73e8342",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "\n",
    "cfg.OUTPUT_DIR = 'Rotated-MaskRCNN'\n",
    "\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "\n",
    "cfg.DATASETS.TRAIN = (\"CFH_train_average_padded_rotated\",)\n",
    "cfg.DATASETS.TEST = ()\n",
    "\n",
    "cfg.MODEL.MASK_ON=True\n",
    "cfg.MODEL.PROPOSAL_GENERATOR.NAME = \"RRPN\"\n",
    "cfg.MODEL.META_ARCHITECTURE = \"MyGeneralizedRCNN\"\n",
    "cfg.MODEL.RPN.HEAD_NAME = \"StandardRPNHead\"\n",
    "cfg.MODEL.RPN.BBOX_REG_WEIGHTS = (10,10,5,5,1)\n",
    "cfg.MODEL.ANCHOR_GENERATOR.NAME = \"RotatedAnchorGenerator\"\n",
    "cfg.MODEL.ANCHOR_GENERATOR.ANGLES = [[-90,-60,-30,0,30,60,90]]\n",
    "# cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.9 \n",
    "cfg.MODEL.ROI_HEADS.NAME = \"RROIHeads_wMask\"\n",
    "# cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512   #this is far lower than usual.  \n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES =1\n",
    "cfg.MODEL.ROI_BOX_HEAD.POOLER_TYPE = \"ROIAlignRotated\"\n",
    "cfg.MODEL.ROI_MASK_HEAD.POOLER_TYPE = \"ROIAlignRotated\"\n",
    "cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS = (10,10,5,5,1)\n",
    "# cfg.MODEL.ROI_BOX_HEAD.NUM_CONV=4\n",
    "# cfg.MODEL.ROI_MASK_HEAD.NUM_CONV=8\n",
    "cfg.SOLVER.IMS_PER_BATCH = 14 #can be up to  24 for a p100 (6 default)\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD=2500\n",
    "cfg.SOLVER.BASE_LR = 0.0015\n",
    "# cfg.SOLVER.GAMMA=0.5\n",
    "cfg.SOLVER.STEPS=(12500, 14000)\n",
    "cfg.SOLVER.MAX_ITER=15000\n",
    "\n",
    "\n",
    "cfg.DATALOADER.NUM_WORKERS = 1\n",
    "cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS = True \n",
    "# cfg.DATALOADER.SAMPLER_TRAIN= \"RepeatFactorTrainingSampler\"\n",
    "# cfg.DATALOADER.REPEAT_THRESHOLD=0.01\n",
    "# cfg.MODEL.BACKBONE.FREEZE_AT=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e878d12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "# predictor = RotatedPredictor(cfg)\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.9   # set a custom testing threshold\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f06302",
   "metadata": {},
   "source": [
    "# Visualize Model Output and Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f32d758",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in random.sample(rbbox_train_dicts, 3):\n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
    "    v = myVisualizer(im[:, :, ::-1],\n",
    "                  metadata=MetadataCatalog.get(\"Test\"), \n",
    "                  scale=0.5,\n",
    "                  instance_mode=ColorMode.IMAGE_BW)   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    "    # )\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    plt.imshow(out.get_image()[:, :, ::-1])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
